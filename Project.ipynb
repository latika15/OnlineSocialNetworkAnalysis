{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import hashlib\n",
    "import io\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import datetime\n",
    "from TwitterAPI import TwitterAPI\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160 traffic files found\n"
     ]
    }
   ],
   "source": [
    "def get_data():\n",
    "    path = 'data'\n",
    "    filenames = []\n",
    "    for filename in os.listdir(path):\n",
    "        if \".csv\" in filename:\n",
    "            filenames.append(path + os.sep +filename)\n",
    "    filenames = sorted(filenames)\n",
    "    \n",
    "    return sorted(filenames)\n",
    "\n",
    "filenames = get_data()\n",
    "print '%d traffic files found' % len(filenames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4640\n"
     ]
    }
   ],
   "source": [
    "def get_file_data(filenames):\n",
    "    traffic_data = []\n",
    "    for filename in filenames:\n",
    "        f = open(filename, 'rb')\n",
    "        reader = csv.reader(f)\n",
    "        row_count = -1\n",
    "        for row in reader:\n",
    "            if row_count != -1:\n",
    "                traffic_data.append(row)\n",
    "            row_count += 1\n",
    "        f.close()\n",
    "    \n",
    "    return traffic_data\n",
    "\n",
    "traffic_data = get_file_data(filenames)\n",
    "print len(traffic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rogers Park - West Ridge',\n",
       " '1',\n",
       " '-87.709645',\n",
       " '-87.654561',\n",
       " '41.997946',\n",
       " '42.026444',\n",
       " 'North of Devon. Kedzie to Lake Shore',\n",
       " '20.45',\n",
       " '2015-10-31 16:50:40.0']"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2015-10-31 16:50:40.0'"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CURRENT_SPEED = 7\n",
    "LAST_UPDATED = 8\n",
    "traffic_data[0][LAST_UPDATED]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2015, 10, 31, 16, 50, 40)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert 'LAST_UPDATED' param of traffic_data to a datetime object and return in format YYYY,MM,DD,H,M,S\n",
    "def get_date_obj(date_string):\n",
    "    date_string = date_string[:-2]\n",
    "    date_obj = datetime.datetime.strptime(date_string, \"%Y-%m-%d %H:%M:%S\")    \n",
    "    return date_obj\n",
    "get_date_obj(traffic_data[0][LAST_UPDATED])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#divide the tweets according to time. morning: 0700 - 1100, noon 1100 - 1400, evening 1400 - 1700, night: 1700-2100\n",
    "def divide_trafficdata_bytime(traffic_data,LAST_UPDATED):\n",
    "    morning = []\n",
    "    noon = []\n",
    "    evening = []\n",
    "    night = []\n",
    "    late_night = []\n",
    "    traffic_bytime = {}\n",
    "    for row in traffic_data:\n",
    "        date = get_date_obj(row[LAST_UPDATED])\n",
    "        if date.time().hour >= 7 and date.time().hour < 11:\n",
    "            morning.append(row)\n",
    "        elif date.time().hour >= 11 and date.time().hour < 15:\n",
    "            noon.append(row)\n",
    "        elif date.time().hour >= 15 and date.time().hour < 17:\n",
    "            evening.append(row)\n",
    "        elif date.time().hour >= 17 and date.time().hour < 21:\n",
    "            night.append(row)\n",
    "        else:\n",
    "            late_night.append(row)\n",
    "            \n",
    "    traffic_bytime['morning'] = morning\n",
    "    traffic_bytime['noon'] = noon\n",
    "    traffic_bytime['evening'] = evening\n",
    "    traffic_bytime['night'] = night\n",
    "    traffic_bytime['late_night'] = late_night\n",
    "    \n",
    "    return traffic_bytime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.1714389102\n",
      "0.0\n",
      "173.86\n",
      "23.2482327586\n",
      "0.0\n",
      "41.86\n",
      "20.8666190076\n",
      "10.23\n",
      "43.64\n",
      "21.3287715517\n",
      "0.0\n",
      "37.94\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def get_average_speed(traffic_data,CURRENT_SPEED):\n",
    "    \n",
    "    speed = [row[CURRENT_SPEED] for row in traffic_data]\n",
    "    speed = np.array(speed,dtype='float')\n",
    "    print np.mean(speed)\n",
    "    print np.min(speed)\n",
    "    print np.max(speed)\n",
    "get_average_speed(traffic_bytime['morning'],CURRENT_SPEED)\n",
    "get_average_speed(traffic_bytime['noon'],CURRENT_SPEED)\n",
    "get_average_speed(traffic_bytime['evening'],CURRENT_SPEED)\n",
    "get_average_speed(traffic_bytime['night'],CURRENT_SPEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**TWITTER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SSLError",
     "evalue": "EOF occurred in violation of protocol (_ssl.c:590)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSSLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-294-16ae1ccf478b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Replace the API_KEY and API_SECRET with your application's key and secret.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mauth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAppAuthHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'aTpLPRCEcym54l0kgzTHH8VTt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'jbXrNKePxA11hgY0y1fHHfphNH9wBKbymnmgUaMHhiprIEhFzG'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mapi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAPI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mauth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwait_on_rate_limit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwait_on_rate_limit_notify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Latika\\Anaconda\\lib\\site-packages\\tweepy\\auth.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, consumer_key, consumer_secret)\u001b[0m\n\u001b[0;32m    164\u001b[0m                              auth=(self.consumer_key,\n\u001b[0;32m    165\u001b[0m                                    self.consumer_secret),\n\u001b[1;32m--> 166\u001b[1;33m                              data={'grant_type': 'client_credentials'})\n\u001b[0m\u001b[0;32m    167\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'token_type'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'bearer'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Latika\\Anaconda\\lib\\site-packages\\requests\\api.pyc\u001b[0m in \u001b[0;36mpost\u001b[1;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    107\u001b[0m     \"\"\"\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'post'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Latika\\Anaconda\\lib\\site-packages\\requests\\api.pyc\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m     \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m     \u001b[1;31m# By explicitly closing the session, we avoid leaving sockets open which\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;31m# can trigger a ResourceWarning in some cases, and look like a memory leak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Latika\\Anaconda\\lib\\site-packages\\requests\\sessions.pyc\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    463\u001b[0m         }\n\u001b[0;32m    464\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 465\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Latika\\Anaconda\\lib\\site-packages\\requests\\sessions.pyc\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 573\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    574\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Latika\\Anaconda\\lib\\site-packages\\requests\\adapters.pyc\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    429\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_SSLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_HTTPError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_SSLError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 431\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    432\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mReadTimeoutError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mReadTimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSSLError\u001b[0m: EOF occurred in violation of protocol (_ssl.c:590)"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "\n",
    "# Replace the API_KEY and API_SECRET with your application's key and secret.\n",
    "auth = tweepy.AppAuthHandler('aTpLPRCEcym54l0kgzTHH8VTt', 'jbXrNKePxA11hgY0y1fHHfphNH9wBKbymnmgUaMHhiprIEhFzG')\n",
    " \n",
    "api = tweepy.API(auth, wait_on_rate_limit=True,wait_on_rate_limit_notify=True)\n",
    " \n",
    "if (not api):\n",
    "    print (\"Can't Authenticate\")\n",
    "    sys.exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading max 10000000 tweets\n",
      "Downloaded 100 tweets\n",
      "Downloaded 200 tweets\n",
      "Downloaded 300 tweets\n",
      "Downloaded 400 tweets\n",
      "Downloaded 500 tweets\n",
      "Downloaded 600 tweets\n",
      "Downloaded 700 tweets\n",
      "Downloaded 800 tweets\n",
      "Downloaded 900 tweets\n",
      "Downloaded 1000 tweets\n",
      "Downloaded 1100 tweets\n",
      "Downloaded 1200 tweets\n",
      "Downloaded 1300 tweets\n",
      "Downloaded 1400 tweets\n",
      "Downloaded 1500 tweets\n",
      "Downloaded 1600 tweets\n",
      "Downloaded 1700 tweets\n",
      "Downloaded 1800 tweets\n",
      "Downloaded 1900 tweets\n",
      "Downloaded 2000 tweets\n",
      "Downloaded 2100 tweets\n",
      "Downloaded 2200 tweets\n",
      "Downloaded 2300 tweets\n",
      "Downloaded 2400 tweets\n",
      "Downloaded 2500 tweets\n",
      "Downloaded 2600 tweets\n",
      "Downloaded 2700 tweets\n",
      "Downloaded 2800 tweets\n",
      "Downloaded 2900 tweets\n",
      "Downloaded 3000 tweets\n",
      "Downloaded 3100 tweets\n",
      "Downloaded 3200 tweets\n",
      "Downloaded 3300 tweets\n",
      "Downloaded 3400 tweets\n",
      "Downloaded 3428 tweets\n",
      "No more tweets found\n",
      "Downloaded 3428 tweets, Saved to tweets.txt\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import jsonpickle\n",
    "import os\n",
    "\n",
    "searchQuery = 'chicago traffic'  # this is what we're searching for\n",
    "maxTweets = 10000000 # Some arbitrary large number\n",
    "tweetsPerQry = 100  # this is the max the API permits\n",
    "fName = 'tweets.txt' # We'll store the tweets in a text file.\n",
    "\n",
    "\n",
    "# If results from a specific ID onwards are reqd, set since_id to that ID.\n",
    "# else default to no lower limit, go as far back as API allows\n",
    "sinceId = None\n",
    "\n",
    "# If results only below a specific ID are, set max_id to that ID.\n",
    "# else default to no upper limit, start from the most recent tweet matching the search query.\n",
    "max_id = -1L\n",
    "\n",
    "tweetCount = 0\n",
    "print(\"Downloading max {0} tweets\".format(maxTweets))\n",
    "with open(fName, 'w') as f:\n",
    "    while tweetCount < maxTweets:\n",
    "        try:\n",
    "            if (max_id <= 0):\n",
    "                if (not sinceId):\n",
    "                    new_tweets = api.search(q=searchQuery, count=tweetsPerQry)\n",
    "                else:\n",
    "                    new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
    "                                            since_id=sinceId)\n",
    "            else:\n",
    "                if (not sinceId):\n",
    "                    new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
    "                                            max_id=str(max_id - 1))\n",
    "                else:\n",
    "                    new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
    "                                            max_id=str(max_id - 1),\n",
    "                                            since_id=sinceId)\n",
    "            if not new_tweets:\n",
    "                print(\"No more tweets found\")\n",
    "                break\n",
    "            for tweet in new_tweets:\n",
    "                f.write(jsonpickle.encode(tweet._json, unpicklable=False) +\n",
    "                        '\\n')\n",
    "            tweetCount += len(new_tweets)\n",
    "            print(\"Downloaded {0} tweets\".format(tweetCount))\n",
    "            max_id = new_tweets[-1].id\n",
    "        except tweepy.TweepError as e:\n",
    "            # Just exit if any error\n",
    "            print(\"some error : \" + str(e))\n",
    "            break\n",
    "\n",
    "print (\"Downloaded {0} tweets, Saved to {1}\".format(tweetCount, fName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading max 10000000 tweets\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tweepy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-292-b828661e0dfa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Downloaded {0} tweets\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweetCount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0mmax_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_tweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[1;32mexcept\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTweepError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m             \u001b[1;31m# Just exit if any error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"some error : \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tweepy' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "searchQuery = 'chicago traffic'  # this is what we're searching for\n",
    "maxTweets = 10000000 # Some arbitrary large number\n",
    "tweetsPerQry = 100  # this is the max the API permits\n",
    "geocode = '42.026444,-87.654561,2mi'\n",
    "fName = 'tweets_geo.txt' # We'll store the tweets in a text file.\n",
    "\n",
    "\n",
    "# If results from a specific ID onwards are reqd, set since_id to that ID.\n",
    "# else default to no lower limit, go as far back as API allows\n",
    "sinceId = None\n",
    "\n",
    "# If results only below a specific ID are, set max_id to that ID.\n",
    "# else default to no upper limit, start from the most recent tweet matching the search query.\n",
    "max_id = -1L\n",
    "\n",
    "tweetCount = 0\n",
    "print(\"Downloading max {0} tweets\".format(maxTweets))\n",
    "with open(fName, 'w') as f:\n",
    "    while tweetCount < maxTweets:\n",
    "        try:\n",
    "            if (max_id <= 0):\n",
    "                if (not sinceId):\n",
    "                    new_tweets = api.search(q=searchQuery, geocode = geocode,count=tweetsPerQry)\n",
    "                else:\n",
    "                    new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
    "                                            since_id=sinceId)\n",
    "            else:\n",
    "                if (not sinceId):\n",
    "                    new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
    "                                            max_id=str(max_id - 1))\n",
    "                else:\n",
    "                    new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
    "                                            max_id=str(max_id - 1),\n",
    "                                            since_id=sinceId)\n",
    "            if not new_tweets:\n",
    "                print(\"No more tweets found\")\n",
    "                break\n",
    "            for tweet in new_tweets:\n",
    "                f.write(jsonpickle.encode(tweet._json, unpicklable=False) +\n",
    "                        '\\n')\n",
    "            tweetCount += len(new_tweets)\n",
    "            print(\"Downloaded {0} tweets\".format(tweetCount))\n",
    "            max_id = new_tweets[-1].id\n",
    "        except tweepy.TweepError as e:\n",
    "            # Just exit if any error\n",
    "            print(\"some error : \" + str(e))\n",
    "            break\n",
    "\n",
    "print (\"Downloaded {0} tweets, Saved to {1}\".format(tweetCount, fName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'contributors': None, u'truncated': False, u'text': u'The Latest: Chicago schools prepare lesson plan on shooting syndication.ap.orgProtesters block rush hour traffic a\\u2026 https://t.co/zCOPRBv4BN', u'is_quote_status': False, u'in_reply_to_status_id': None, u'in_reply_to_user_id': None, u'id': 669942638393585664L, u'favorite_count': 0, u'entities': {u'symbols': [], u'user_mentions': [], u'hashtags': [], u'urls': [{u'indices': [116, 139], u'url': u'https://t.co/zCOPRBv4BN', u'expanded_url': u'http://ift.tt/1kVrO7G', u'display_url': u'ift.tt/1kVrO7G'}]}, u'retweeted': False, u'coordinates': None, u'source': u'<a href=\"http://ifttt.com\" rel=\"nofollow\">IFTTT</a>', u'in_reply_to_screen_name': None, u'id_str': u'669942638393585664', u'retweet_count': 0, u'metadata': {u'iso_language_code': u'en', u'result_type': u'recent'}, u'favorited': False, u'user': {u'follow_request_sent': None, u'has_extended_profile': False, u'profile_use_background_image': True, u'id': 3270783145L, u'verified': False, u'profile_text_color': u'333333', u'profile_image_url_https': u'https://pbs.twimg.com/profile_images/618367130598158336/gZZe9zH9_normal.jpg', u'profile_sidebar_fill_color': u'DDEEF6', u'entities': {u'url': {u'urls': [{u'indices': [0, 23], u'url': u'https://t.co/aMrywGb1fg', u'expanded_url': u'https://twitter.com/faridabadtutor', u'display_url': u'twitter.com/faridabadtutor'}]}, u'description': {u'urls': []}}, u'followers_count': 338, u'protected': False, u'location': u'Ballabgarh, Faridabad, Haryana', u'default_profile_image': False, u'id_str': u'3270783145', u'lang': u'en', u'utc_offset': -28800, u'statuses_count': 346586, u'description': u'Female Tutor for Tuition for all classes in Faridabad 9-10th Maths,  Abacus Coaching Chawla Colony #Ballabgarh #Faridabad #Haryana Call:9953732265', u'friends_count': 9, u'profile_background_image_url_https': u'https://abs.twimg.com/images/themes/theme1/bg.png', u'profile_link_color': u'0084B4', u'profile_image_url': u'http://pbs.twimg.com/profile_images/618367130598158336/gZZe9zH9_normal.jpg', u'following': None, u'geo_enabled': False, u'profile_background_color': u'C0DEED', u'profile_background_image_url': u'http://abs.twimg.com/images/themes/theme1/bg.png', u'screen_name': u'faridabadtutor', u'is_translation_enabled': True, u'profile_background_tile': False, u'favourites_count': 8, u'name': u'Faridabad Tuitions', u'notifications': None, u'url': u'https://t.co/aMrywGb1fg', u'created_at': u'Tue Jul 07 06:48:05 +0000 2015', u'contributors_enabled': False, u'time_zone': u'Pacific Time (US & Canada)', u'profile_sidebar_border_color': u'C0DEED', u'default_profile': True, u'is_translator': False, u'listed_count': 131}, u'geo': None, u'in_reply_to_user_id_str': None, u'possibly_sensitive': False, u'lang': u'en', u'created_at': u'Thu Nov 26 18:15:46 +0000 2015', u'in_reply_to_status_id_str': None, u'place': None}\n"
     ]
    }
   ],
   "source": [
    "import jsonpickle\n",
    "f = open('tweets.txt')\n",
    "all_tweets = []\n",
    "for line in f:\n",
    "    t = jsonpickle.decode(line)\n",
    "    all_tweets.append(t)\n",
    "    \n",
    "print all_tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Latest: Chicago schools prepare lesson plan on shooting syndication.ap.orgProtesters block rush hour traffic aâ€¦ https://t.co/zCOPRBv4BN\n"
     ]
    }
   ],
   "source": [
    "print all_tweets[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6645 unique words in 3428 tweets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'traffic', 2596),\n",
       " (u'chicago', 2140),\n",
       " (u'in', 1841),\n",
       " (u'rt', 1411),\n",
       " (u'the', 1260),\n",
       " (u'at', 920),\n",
       " (u'to', 800),\n",
       " (u'on', 644),\n",
       " (u'#chicago', 598),\n",
       " (u'is', 579)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets[0]['user']['name']\n",
    "all_tweets[0]['user']['location']\n",
    "from collections import Counter\n",
    "counts = Counter()\n",
    "for tweet in all_tweets:\n",
    "    counts.update(tweet['text'].lower().split())\n",
    "print('%d unique words in %d tweets' % (len(counts), len(all_tweets)) )\n",
    "counts.most_common(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(string, lowercase, keep_punctuation,\n",
    "             collapse_urls, collapse_mentions):\n",
    "    \"\"\" Split a tweet into tokens.\"\"\"\n",
    "    if not string:\n",
    "        return []\n",
    "    if lowercase:\n",
    "        string = string.lower()\n",
    "    tokens = []\n",
    "    if collapse_urls:\n",
    "        string = re.sub('http\\S+', '', string)\n",
    "    if collapse_mentions:\n",
    "        string = re.sub('@\\S+', '', string)\n",
    "    if keep_punctuation:\n",
    "        tokens = string.split()\n",
    "    else:\n",
    "        tokens = re.sub('\\W+', ' ', string).split()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'the', u'latest', u'chicago', u'schools', u'prepare', u'lesson', u'plan', u'on', u'shooting', u'syndication', u'ap', u'orgprotesters', u'block', u'rush', u'hour', u'traffic', u'a', u'https', u't', u'co', u'zcoprbv4bn']\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "tokens_list = tokenize(all_tweets[0]['text'], lowercase=True,\n",
    "                            keep_punctuation=False,\n",
    "                            collapse_urls=False, collapse_mentions=False)\n",
    "print tokens_list\n",
    "print(len(tokens_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 0.0)\n"
     ]
    }
   ],
   "source": [
    "score = []\n",
    "for tweet in all_tweets:\n",
    "    pscore = 0\n",
    "    nscore = 0\n",
    "    tokens_list = tokenize(tweet['text'], lowercase=True,\n",
    "                            keep_punctuation=False,\n",
    "                            collapse_urls=False, collapse_mentions=False)\n",
    "    for token in tokens_list:\n",
    "        word = swn.senti_synsets(token)\n",
    "        if word:\n",
    "            pscore += word[0].pos_score()\n",
    "            nscore += word[0].neg_score()\n",
    "            score.append((pscore,nscore))\n",
    "    \n",
    "print score[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2015, 11, 26, 18, 15, 46)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert 'created_at' param of tweet to a datetime object and return in format YYYY,MM,DD,H,M,S\n",
    "def get_date_object(tweet):\n",
    "    ti= tweet['created_at'].split()\n",
    "    newt = ti[0]+\" \"+ti[1]+\" \"+ti[2]+\" \"+ti[3]+\" \"+ti[5]\n",
    "    d = datetime.datetime.strptime(newt, \"%a %b %d %H:%M:%S %Y\")    \n",
    "    return d\n",
    "get_date_object(all_tweets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#divide the tweets according to time. morning: 0700 - 1100, noon 1100 - 1400, evening 1400 - 1700, night: 1700-2100\n",
    "\n",
    "def divide_tweets_time(all_tweets):\n",
    "    morning = []\n",
    "    noon = []\n",
    "    evening = []\n",
    "    night = []\n",
    "    late_night = []\n",
    "    tweets_bytime = {}\n",
    "    for tweet in all_tweets:\n",
    "        tweet_date = get_date_object(tweet)\n",
    "        if tweet_date.time().hour >= 7 and tweet_date.time().hour < 11:\n",
    "            morning.append(tweet)\n",
    "        elif tweet_date.time().hour >= 11 and tweet_date.time().hour < 14:\n",
    "            noon.append(tweet)\n",
    "        elif tweet_date.time().hour >= 14 and tweet_date.time().hour < 17:\n",
    "            evening.append(tweet)\n",
    "        elif tweet_date.time().hour >= 17 and tweet_date.time().hour < 21:\n",
    "            night.append(tweet)\n",
    "        else:\n",
    "            late_night.append(tweet)\n",
    "            \n",
    "    tweets_bytime['morning'] = morning\n",
    "    tweets_bytime['noon'] = noon\n",
    "    tweets_bytime['evening'] = evening\n",
    "    tweets_bytime['night'] = night\n",
    "    tweets_bytime['late_night'] = late_night\n",
    "    \n",
    "    return tweets_bytime\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135\n",
      "280\n",
      "399\n",
      "602\n",
      "2012\n",
      "Tue Nov 17 21:14:29 +0000 2015\n"
     ]
    }
   ],
   "source": [
    "tweets_bytime = divide_tweets_time(all_tweets)\n",
    "print len(tweets_bytime['morning'])\n",
    "print len(tweets_bytime['noon'])\n",
    "print len(tweets_bytime['evening'])\n",
    "print len(tweets_bytime['night'])\n",
    "print len(tweets_bytime['late_night'])\n",
    "\n",
    "latenight = tweets_bytime['late_night']\n",
    "print latenight[-1]['created_at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "# Depends on nltk (pip install nltk)\n",
    "# See http://www.nltk.org/data.html\n",
    "happy = swn.senti_synsets('good traffic','a')\n",
    "print happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================\n",
      "0.125 2.125 [u'chicago', u'cops', u'so', u'above', u'the', u'law', u'they', u'r', u'willing', u'2', u'abuse', u'even', u'those', u'who', u'investigate', u'them', u'why', u'can', u't', u'bad', u'cops', u'be', u'fired'] Thu Nov 26 18:15:46 +0000 2015\n",
      "======================\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-a73f6e04cd3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     text = tweet2tokens(t, lowercase=True,\n\u001b[0;32m      4\u001b[0m                             \u001b[0mkeep_punctuation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                             collapse_urls=True, collapse_mentions=True)\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mpscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mnscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-73-d3cbb66320b8>\u001b[0m in \u001b[0;36mtweet2tokens\u001b[1;34m(tweet, lowercase, keep_punctuation, collapse_urls, collapse_mentions)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \"\"\" Convert a tweet into a list of tokens, from the tweet text and optionally the\n\u001b[0;32m      5\u001b[0m     user description. \"\"\"\n\u001b[1;32m----> 6\u001b[1;33m     tokens = tokenize(tweet['text'], lowercase, keep_punctuation,\n\u001b[0m\u001b[0;32m      7\u001b[0m                        collapse_urls, collapse_mentions)\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "for tweet in all_tweets:\n",
    "    print '======================'\n",
    "    text = tweet2tokens(t, lowercase=True,\n",
    "                            keep_punctuation=False,\n",
    "                            collapse_urls=True, collapse_mentions=True)\n",
    "    pscore = 0\n",
    "    nscore = 0\n",
    "    for t in text:\n",
    "        word = swn.senti_synsets(t)\n",
    "        if word:\n",
    "            pscore += word[0].pos_score()\n",
    "            nscore += word[0].neg_score()\n",
    "    print pscore,nscore,text,tweet['created_at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-133-f3ed8fcbbd79>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mpos_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneg_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msenti_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolarity_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;31m#if pos_score != 0.0 or neg_score!=0.0:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0mpos_score\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mneg_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Ishan\\Anaconda\\lib\\site-packages\\senti_classifier\\senti_classifier.py\u001b[0m in \u001b[0;36mpolarity_scores\u001b[1;34m(lines_list)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpolarity_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlines_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m     \u001b[0mpos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlines_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msynsets_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbag_of_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Ishan\\Anaconda\\lib\\site-packages\\senti_classifier\\senti_classifier.py\u001b[0m in \u001b[0;36mclassify\u001b[1;34m(text, synsets_scores, bag_of_words)\u001b[0m\n\u001b[0;32m    185\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mdisambiguateWordSenses\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m                     \u001b[0mdisamb_syn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdisambiguateWordSenses\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0msynsets_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhas_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdisamb_syn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m                         \u001b[1;31m#uncomment the disamb_syn.split... if also want to check synsets polarity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Ishan\\Anaconda\\lib\\site-packages\\senti_classifier\\senti_classifier.py\u001b[0m in \u001b[0;36mdisambiguateWordSenses\u001b[1;34m(sentence, word)\u001b[0m\n\u001b[0;32m    156\u001b[0m            \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m            \u001b[1;32mfor\u001b[0m \u001b[0mwsynset\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m                \u001b[0msim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwsynset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msynset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m                \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m                    \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Ishan\\Anaconda\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py\u001b[0m in \u001b[0;36mpath_similarity\u001b[1;34m(self, synset1, synset2, verbose, simulate_root)\u001b[0m\n\u001b[0;32m   1547\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1548\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpath_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msynset1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msynset2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msimulate_root\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1549\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msynset1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msynset2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msimulate_root\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1550\u001b[0m     \u001b[0mpath_similarity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSynset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath_similarity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1551\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Ishan\\Anaconda\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py\u001b[0m in \u001b[0;36mpath_similarity\u001b[1;34m(self, other, verbose, simulate_root)\u001b[0m\n\u001b[0;32m    753\u001b[0m         \"\"\"\n\u001b[0;32m    754\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 755\u001b[1;33m         \u001b[0mdistance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshortest_path_distance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msimulate_root\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msimulate_root\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_needs_root\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    756\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdistance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdistance\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Ishan\\Anaconda\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py\u001b[0m in \u001b[0;36mshortest_path_distance\u001b[1;34m(self, other, simulate_root)\u001b[0m\n\u001b[0;32m    674\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[0mdist_dict1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shortest_hypernym_paths\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msimulate_root\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m         \u001b[0mdist_dict2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shortest_hypernym_paths\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msimulate_root\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Ishan\\Anaconda\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py\u001b[0m in \u001b[0;36m_shortest_hypernym_paths\u001b[1;34m(self, simulate_root)\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mqueue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m             \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 643\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    644\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m             \u001b[0mpath\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Ishan\\Anaconda\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py\u001b[0m in \u001b[0;36m__hash__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_related\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'&'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhash\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from senti_classifier import senti_classifier\n",
    "t = all_tweets[0:100]\n",
    "for tweet in t:\n",
    "    sentence = tweet['text']\n",
    "    pos_score, neg_score = senti_classifier.polarity_scores(sentence)\n",
    "    #if pos_score != 0.0 or neg_score!=0.0:\n",
    "    print pos_score,neg_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
